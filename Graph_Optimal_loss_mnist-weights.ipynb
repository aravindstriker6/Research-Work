{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import queue\n",
    "import time\n",
    "\n",
    "from utils.data_utils import load_dataset_numpy\n",
    "\n",
    "import scipy.spatial.distance\n",
    "\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.sparse.csgraph import maximum_flow\n",
    "from utils.flow import _make_edge_pointers\n",
    "\n",
    "from cvxopt import solvers, matrix, spdiag, log, mul, sparse, spmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minll(G,h,p):\n",
    "    m,v_in=G.size\n",
    "    def F(x=None,z=None):\n",
    "        if x is None:\n",
    "            return 0, matrix(1.0,(v,1))\n",
    "        if min(x)<=0.0:\n",
    "            return None\n",
    "        f = -sum(mul(p,log(x)))\n",
    "        Df = mul(p.T,-(x**-1).T)\n",
    "        if z is None:\n",
    "            return f,Df\n",
    "        # Fix the Hessian\n",
    "        H = spdiag(z[0]*mul(p,x**-2))\n",
    "        return f,Df,H\n",
    "    return solvers.cp(F,G=G,h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_remaining_cap_edges(edge_ptr,capacities,heads,tails, source, sink):\n",
    "    ITYPE = np.int32\n",
    "    n_verts = edge_ptr.shape[0] - 1\n",
    "    n_edges = capacities.shape[0]\n",
    "    ITYPE_MAX = np.iinfo(ITYPE).max\n",
    "\n",
    "    # Our result array will keep track of the flow along each edge\n",
    "    flow = np.zeros(n_edges, dtype=ITYPE)\n",
    "\n",
    "    # Create a circular queue for breadth-first search. Elements are\n",
    "    # popped dequeued at index start and queued at index end.\n",
    "    q = np.empty(n_verts, dtype=ITYPE)\n",
    "\n",
    "    # Create an array indexing predecessor edges\n",
    "    pred_edge = np.empty(n_verts, dtype=ITYPE)\n",
    "\n",
    "    # While augmenting paths from source to sink exist\n",
    "    for k in range(n_verts):\n",
    "        pred_edge[k] = -1\n",
    "    path_edges = []\n",
    "    # Reset queue to consist only of source\n",
    "    q[0] = source\n",
    "    start = 0\n",
    "    end = 1\n",
    "    # While we have not found a path, and queue is not empty\n",
    "    path_found = False\n",
    "    while start != end and not path_found:\n",
    "        # Pop queue\n",
    "        cur = q[start]\n",
    "        start += 1\n",
    "        if start == n_verts:\n",
    "            start = 0\n",
    "        # Loop over all edges from the current vertex\n",
    "        for e in range(edge_ptr[cur], edge_ptr[cur + 1]):\n",
    "            t = heads[e]\n",
    "            if pred_edge[t] == -1 and t != source and\\\n",
    "                    capacities[e] > flow[e]:\n",
    "                pred_edge[t] = e\n",
    "                path_edges.append((cur,t))\n",
    "                if t == sink:\n",
    "                    path_found = True\n",
    "                    break\n",
    "                # Push to queue\n",
    "                q[end] = t\n",
    "                end += 1\n",
    "                if end == n_verts:\n",
    "                    end = 0\n",
    "    return path_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_rep(edge_matrix,n_1,n_2):\n",
    "    graph_rep = []\n",
    "    for i in range(n_1+n_2+2):\n",
    "        graph_rep.append([])\n",
    "        if i==0:\n",
    "            #source\n",
    "            for j in range(n_1+n_2+2):\n",
    "                if j==0:\n",
    "                    graph_rep[i].append(0)\n",
    "                elif 1<=j<=n_1:\n",
    "                    graph_rep[i].append(1)\n",
    "                elif n_1<j<=n_1+n_2+1:\n",
    "                    graph_rep[i].append(0)\n",
    "        elif 1<=i<=n_1:\n",
    "            # LHS vertices\n",
    "            for j in range(n_1+n_2+2):\n",
    "                if j<=n_1:\n",
    "                    graph_rep[i].append(0)\n",
    "                elif n_1<j<=n_1+n_2:\n",
    "                    if edge_matrix[i-1,j-n_1-1]:\n",
    "                        graph_rep[i].append(1)\n",
    "                    else:\n",
    "                        graph_rep[i].append(0)\n",
    "                elif n_1+n_2<j:\n",
    "                    graph_rep[i].append(0)\n",
    "        elif n_1<i<=n_1+n_2:\n",
    "            #RHS vertices\n",
    "            for j in range(n_1+n_2+2):\n",
    "                if j<=n_1+n_2:\n",
    "                    graph_rep[i].append(0)\n",
    "                elif j>n_1+n_2:\n",
    "                    graph_rep[i].append(1)\n",
    "        elif i==n_1+n_2+1:\n",
    "            #Sink\n",
    "            for j in range(n_1+n_2+2):\n",
    "                graph_rep[i].append(0)\n",
    "\n",
    "    graph_rep_array=np.array(graph_rep)\n",
    "\n",
    "    return graph_rep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classifier_prob_full_flow(top_level_vertices,w_1_curr,w_2_curr):\n",
    "    for item in top_level_vertices:\n",
    "        if item !=0 and item != sink_idx:\n",
    "            classifier_probs[item-1,0]=w_1_curr/(w_1_curr+w_2_curr)\n",
    "            classifier_probs[item-1,1]=w_2_curr/(w_1_curr+w_2_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classifier_prob_no_flow(top_level_vertices):\n",
    "    for item in top_level_vertices:\n",
    "        if item !=0 and item != sink_idx:\n",
    "            if item<=n_1:\n",
    "                classifier_probs[item-1,0]=1\n",
    "                classifier_probs[item-1,1]=0\n",
    "            elif item>n_1:\n",
    "                classifier_probs[item-1,0]=0\n",
    "                classifier_probs[item-1,1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_rescale(graph_rep_curr,top_level_indices,weights):\n",
    "    class_1_vertices=top_level_indices[( top_level_indices> 0) & (top_level_indices<=n_1)]\n",
    "    class_1_weights=weights[class_1_vertices]\n",
    "    w_1_curr=np.sum(class_1_weights)\n",
    "    class_2_vertices=top_level_indices[ (top_level_indices>n_1) & (top_level_indices<n_1+n_2)]\n",
    "    class_2_weights=weights[class_2_vertices]\n",
    "    w_2_curr=np.sum(class_2_weights)\n",
    "    n_1_curr=len(class_1_vertices)\n",
    "    n_2_curr=len(class_2_vertices)\n",
    "    #n_1_curr=len(np.where(top_level_indices<=n_1)[0])-1\n",
    "    #n_2_curr=len(np.where(top_level_indices>n_1)[0])-1\n",
    "    # source rescale\n",
    "    # print(graph_rep_curr[0])\n",
    "    #graph_rep_curr[0,:]=graph_rep_curr[0,:]/n_2\n",
    "    graph_rep_curr[0,:]= (graph_rep_curr[0,:]*w_2_curr)\n",
    "    # print(graph_rep_curr[0])\n",
    "    # bipartite graph edge scale\n",
    "    #graph_rep_curr[1:n_1_curr+1,:]=graph_rep_curr[1:n_1_curr+1,:]/(n_1*n_2)\n",
    "    graph_rep_curr[1:n_1_curr+1,:]=(graph_rep_curr[1:n_1_curr+1,:]*(w_1_curr*w_2_curr))\n",
    "    # sink edges rescale\n",
    "    #graph_rep_curr[n_1_curr+1:,:]=graph_rep_curr[n_1_curr+1:,:]/n_1\n",
    "    graph_rep_curr[n_1_curr+1:,:]=(graph_rep_curr[n_1_curr+1:,:]*w_1_curr)\n",
    "    return graph_rep_curr,w_1_curr,w_2_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_flow_and_split(top_level_indices,weights):\n",
    "    top_level_indices_1=None\n",
    "    top_level_indices_2=None\n",
    "    #Create subgraph from index array provided\n",
    "    graph_rep_curr = graph_rep_array[top_level_indices]\n",
    "    graph_rep_curr = graph_rep_curr[:,top_level_indices]\n",
    "    graph_rep_curr,w_1_curr,w_2_curr = graph_rescale(graph_rep_curr,top_level_indices,weights)\n",
    "    graph_curr=csr_matrix(graph_rep_curr)\n",
    "    flow_curr = maximum_flow(graph_curr,0,len(top_level_indices)-1)\n",
    "    # Checking if full flow occurred, so no need to split\n",
    "    if flow_curr.flow_value==w_1_curr*w_2_curr:\n",
    "        set_classifier_prob_full_flow(top_level_indices,w_1_curr,w_2_curr)\n",
    "        return top_level_indices_1,top_level_indices_2, flow_curr\n",
    "    elif flow_curr.flow_value==0:\n",
    "        set_classifier_prob_no_flow(top_level_indices)\n",
    "        return top_level_indices_1,top_level_indices_2, flow_curr\n",
    "    # Finding remaining capacity edges\n",
    "    remainder_array = graph_curr-flow_curr.residual\n",
    "\n",
    "    rev_edge_ptr, tails = _make_edge_pointers(remainder_array)\n",
    "\n",
    "    edge_ptr=remainder_array.indptr\n",
    "    capacities=remainder_array.data\n",
    "    heads=remainder_array.indices\n",
    "\n",
    "    edge_list_curr = find_remaining_cap_edges(edge_ptr,capacities,heads,tails,0,len(top_level_indices)-1)\n",
    "\n",
    "#     print(edge_list_curr)\n",
    "    gz_idx = []\n",
    "    for item in edge_list_curr:\n",
    "        gz_idx.append(item[0])\n",
    "        gz_idx.append(item[1])\n",
    "    if len(gz_idx)>0:\n",
    "        gz_idx=np.array(gz_idx)\n",
    "        gz_idx_unique=np.unique(gz_idx)\n",
    "        top_level_gz_idx=top_level_indices[gz_idx_unique]\n",
    "        top_level_gz_idx=np.insert(top_level_gz_idx,len(top_level_gz_idx),sink_idx)\n",
    "        top_level_indices_1=top_level_gz_idx\n",
    "    else:\n",
    "        top_level_gz_idx=np.array([0,sink_idx])\n",
    "    # Indices without flow\n",
    "    top_level_z_idx=np.setdiff1d(top_level_indices,top_level_gz_idx)\n",
    "    if len(top_level_z_idx)>0:\n",
    "        # Add source and sink back to zero flow idx array\n",
    "        top_level_z_idx=np.insert(top_level_z_idx,0,0)\n",
    "        top_level_z_idx=np.insert(top_level_z_idx,len(top_level_z_idx),sink_idx)\n",
    "        top_level_indices_2=top_level_z_idx\n",
    "    \n",
    "    return top_level_indices_1,top_level_indices_2, flow_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--class_2'], dest='class_2', nargs=None, const=None, default=7, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\"--dataset_in\", default='MNIST',\n",
    "                    help=\"dataset to be used\")\n",
    "parser.add_argument(\"--norm\", default='l2',\n",
    "                    help=\"norm to be used\")\n",
    "parser.add_argument('--num_samples', type=int, default=None)\n",
    "parser.add_argument('--n_classes', type=int, default=2)\n",
    "parser.add_argument('--eps', type=float, default=None)\n",
    "parser.add_argument('--approx_only', dest='approx_only', action='store_true')\n",
    "parser.add_argument('--use_test', dest='use_test', action='store_true')\n",
    "parser.add_argument('--track_hard', dest='track_hard', action='store_true')\n",
    "parser.add_argument('--use_full', dest='use_full', action='store_true')\n",
    "parser.add_argument('--run_generic', dest='run_generic', action='store_true')\n",
    "parser.add_argument('--new_marking_strat', type=str, default=None)\n",
    "parser.add_argument('--num_reps', type=int, default=2)\n",
    "parser.add_argument('--class_1', type=int, default=3)\n",
    "parser.add_argument('--class_2', type=int, default=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--dataset_in=MNIST --num_samples=500 --use_full --eps=4\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(approx_only=False, class_1=3, class_2=7, dataset_in='MNIST', eps=4.0, n_classes=2, new_marking_strat=None, norm='l2', num_reps=2, num_samples=500, run_generic=False, track_hard=False, use_full=True, use_test=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, data_details = load_dataset_numpy(args, data_dir='data',\n",
    "                                                        training_time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 1000\n",
       "    Root location: data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 1000\n",
       "    Root location: data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_channels': 1, 'h_in': 28, 'w_in': 28, 'scale': 255.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "DATA_DIM = data_details['n_channels']*data_details['h_in']*data_details['w_in']\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Pytorch normalizes tensors (so need manual here!)\n",
    "if args.use_test:\n",
    "    for (x,y,_, _, _) in test_data:\n",
    "        X.append(x/255.)\n",
    "        Y.append(y)\n",
    "else:\n",
    "    for (x,y,_, _, _) in train_data:\n",
    "        X.append(x/255.)\n",
    "        Y.append(y)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "num_samples = int(len(X)/2)\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c1 = X[:num_samples].reshape(num_samples, DATA_DIM)\n",
    "X_c2 = X[num_samples:].reshape(num_samples, DATA_DIM)\n",
    "\n",
    "class_1 = args.class_1\n",
    "class_2 = args.class_2\n",
    "\n",
    "if not os.path.exists('distances'):\n",
    "    os.makedirs('distances')\n",
    "\n",
    "if not os.path.exists('cost_results'):\n",
    "    os.makedirs('cost_results')\n",
    "\n",
    "if args.use_full:\n",
    "    subsample_sizes = [args.num_samples]\n",
    "else:\n",
    "    subsample_sizes = [args.num_samples]\n",
    "   \n",
    "\n",
    "rng = np.random.default_rng(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=[0,1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:74: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current queue size at eps 0 is 1\n",
      "Log loss for eps 0 is -0.0\n",
      "0 1\n",
      "Current queue size at eps 1 is 1\n",
      "Log loss for eps 1 is -0.0\n",
      "0 2\n",
      "Current queue size at eps 2 is 1\n",
      "Log loss for eps 2 is -0.0\n",
      "86 3\n",
      "Current queue size at eps 3 is 1\n",
      "Current queue size at eps 3 is 2\n",
      "Current queue size at eps 3 is 3\n",
      "Current queue size at eps 3 is 4\n",
      "Current queue size at eps 3 is 3\n",
      "Current queue size at eps 3 is 2\n",
      "Current queue size at eps 3 is 3\n",
      "Current queue size at eps 3 is 2\n",
      "Current queue size at eps 3 is 3\n",
      "Current queue size at eps 3 is 4\n",
      "Current queue size at eps 3 is 5\n",
      "Current queue size at eps 3 is 6\n",
      "Current queue size at eps 3 is 5\n",
      "Current queue size at eps 3 is 4\n",
      "Current queue size at eps 3 is 3\n",
      "Current queue size at eps 3 is 2\n",
      "Current queue size at eps 3 is 1\n",
      "Log loss for eps 3 is 0.02257493658474787\n",
      "4483 4\n",
      "Current queue size at eps 4 is 1\n",
      "Current queue size at eps 4 is 2\n",
      "Current queue size at eps 4 is 3\n",
      "Current queue size at eps 4 is 4\n",
      "Current queue size at eps 4 is 3\n",
      "Current queue size at eps 4 is 4\n",
      "Current queue size at eps 4 is 5\n",
      "Current queue size at eps 4 is 6\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 8\n",
      "Current queue size at eps 4 is 9\n",
      "Current queue size at eps 4 is 10\n",
      "Current queue size at eps 4 is 11\n",
      "Current queue size at eps 4 is 12\n",
      "Current queue size at eps 4 is 11\n",
      "Current queue size at eps 4 is 10\n",
      "Current queue size at eps 4 is 11\n",
      "Current queue size at eps 4 is 10\n",
      "Current queue size at eps 4 is 11\n",
      "Current queue size at eps 4 is 12\n",
      "Current queue size at eps 4 is 11\n",
      "Current queue size at eps 4 is 12\n",
      "Current queue size at eps 4 is 13\n",
      "Current queue size at eps 4 is 12\n",
      "Current queue size at eps 4 is 11\n",
      "Current queue size at eps 4 is 10\n",
      "Current queue size at eps 4 is 9\n",
      "Current queue size at eps 4 is 8\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 8\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 6\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 8\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 8\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 8\n",
      "Current queue size at eps 4 is 7\n",
      "Current queue size at eps 4 is 6\n",
      "Current queue size at eps 4 is 5\n",
      "Current queue size at eps 4 is 4\n",
      "Current queue size at eps 4 is 3\n",
      "Current queue size at eps 4 is 2\n",
      "Current queue size at eps 4 is 1\n",
      "Log loss for eps 4 is 0.2712732554159632\n",
      "80376 5\n",
      "Current queue size at eps 5 is 1\n",
      "Current queue size at eps 5 is 2\n",
      "Current queue size at eps 5 is 3\n",
      "Current queue size at eps 5 is 4\n",
      "Current queue size at eps 5 is 3\n",
      "Current queue size at eps 5 is 4\n",
      "Current queue size at eps 5 is 3\n",
      "Current queue size at eps 5 is 2\n",
      "Current queue size at eps 5 is 3\n",
      "Current queue size at eps 5 is 4\n",
      "Current queue size at eps 5 is 3\n",
      "Current queue size at eps 5 is 4\n",
      "Current queue size at eps 5 is 3\n",
      "Current queue size at eps 5 is 2\n",
      "Current queue size at eps 5 is 1\n",
      "Log loss for eps 5 is 0.6630183664918861\n",
      "231292 6\n",
      "Current queue size at eps 6 is 1\n",
      "Log loss for eps 6 is 0.6931476815616832\n",
      "249972 7\n",
      "Current queue size at eps 7 is 1\n",
      "Log loss for eps 7 is 0.6931476815616832\n",
      "250000 8\n",
      "Current queue size at eps 8 is 1\n",
      "Log loss for eps 8 is 0.6931476815616832\n",
      "250000 9\n",
      "Current queue size at eps 9 is 1\n",
      "Log loss for eps 9 is 0.6931476815616832\n",
      "250000 10\n",
      "Current queue size at eps 10 is 1\n",
      "Log loss for eps 10 is 0.6931476815616832\n"
     ]
    }
   ],
   "source": [
    "loss_final=[]\n",
    "for subsample_size in subsample_sizes:\n",
    "  \n",
    "    \n",
    "    loss_list = []\n",
    "    time_list = []\n",
    "    num_edges_list = []\n",
    "\n",
    "    if args.run_generic:\n",
    "        time_generic_list = []\n",
    "\n",
    "    if subsample_size == args.num_samples:\n",
    "        num_reps=1\n",
    "    else:\n",
    "        num_reps=args.num_reps\n",
    "    for rep in range(num_reps):\n",
    "        indices_1 = rng.integers(num_samples,size=subsample_size)\n",
    "        indices_2 = rng.integers(num_samples, size=subsample_size)\n",
    "\n",
    "        if args.use_full:\n",
    "            X_c1_curr = X_c1\n",
    "            X_c2_curr = X_c2\n",
    "        else:\n",
    "            X_c1_curr = X_c1[indices_1]\n",
    "            X_c2_curr = X_c2[indices_2]\n",
    "\n",
    "        if args.use_test:\n",
    "            dist_mat_name = args.dataset_in + '_test_' + str(class_1) + '_' + str(class_2) + '_' + str(subsample_size) + '_' + args.norm + '_rep' + str(rep) + '.npy'\n",
    "        else:\n",
    "            dist_mat_name = args.dataset_in + '_' + str(class_1) + '_' + str(class_2) + '_' + str(subsample_size) + '_' + args.norm + '_rep' + str(rep) + '.npy'\n",
    "        if os.path.exists(dist_mat_name):\n",
    "            print('Loading distances')\n",
    "            D_12 = np.load('distances/' + dist_mat_name)\n",
    "        else:\n",
    "            if args.norm == 'l2':\n",
    "                D_12 = scipy.spatial.distance.cdist(X_c1_curr,X_c2_curr,metric='euclidean')\n",
    "            elif args.norm == 'linf':\n",
    "                D_12 = scipy.spatial.distance.cdist(X_c1_curr,X_c2_curr,metric='chebyshev')\n",
    "            np.save('distances/' + dist_mat_name, D_12)\n",
    "\n",
    "        for m in range(len(eps)):\n",
    "         # Add edge if cost 0\n",
    "         edge_matrix = D_12 <= 2*eps[m]\n",
    "         edge_matrix = edge_matrix.astype(float)\n",
    "\n",
    "         num_edges = len(np.where(edge_matrix!=0)[0])\n",
    "         print(num_edges,eps[m])\n",
    "         num_edges_list.append(num_edges)\n",
    "\n",
    "         n_1=subsample_size\n",
    "         n_2=subsample_size\n",
    "         weights=np.ones(n_1+n_2)\n",
    "\n",
    "         # Create graph representation\n",
    "         graph_rep_array = create_graph_rep(edge_matrix,n_1,n_2)\n",
    "\n",
    "         time1= time.clock()\n",
    "         q = queue.Queue()\n",
    "         # Initial graph indices\n",
    "         q.put(np.arange(n_1+n_2+2))\n",
    "         sink_idx=n_1+n_2+1\n",
    "         count=0\n",
    "         classifier_probs=np.zeros(((n_1)+(n_2),2))\n",
    "         while not q.empty():\n",
    "            print('Current queue size at eps %s is %s' % (eps[m],q.qsize()))\n",
    "            curr_idx_list=q.get()\n",
    "            # print(q.qsize())\n",
    "            list_1, list_2, flow_curr=find_flow_and_split(curr_idx_list,weights)\n",
    "            # print(list_1,list_2,flow_curr.flow_value)\n",
    "            if list_1 is not None:\n",
    "                q.put(list_1)\n",
    "            if list_2 is not None:\n",
    "                q.put(list_2)\n",
    "         time2 = time.clock()\n",
    "\n",
    "         if args.run_generic:\n",
    "            v=(n_1+n_2)\n",
    "            num_edges=len(np.where(edge_matrix==1)[0])\n",
    "            edges=np.where(edge_matrix==1)\n",
    "            incidence_matrix=np.zeros((num_edges,v))\n",
    "\n",
    "            for i in range(num_edges):\n",
    "                j1=edges[0][i]\n",
    "                j2=edges[1][i]+(n_1-1)\n",
    "                incidence_matrix[i,j1]=1\n",
    "                incidence_matrix[i,j2]=1\n",
    "\n",
    "            G_in=np.vstack((incidence_matrix,np.eye(v)))\n",
    "            h_in=np.ones((num_edges+v,1))\n",
    "            p=(1.0/v)*np.ones((v,1))\n",
    "\n",
    "            G_in_sparse_np=coo_matrix(G_in)\n",
    "\n",
    "            G_in_sparse=spmatrix(1.0,G_in_sparse_np.nonzero()[0],G_in_sparse_np.nonzero()[1])\n",
    "\n",
    "            solvers.options['maxiters']=1000\n",
    "\n",
    "            time3=time.clock()\n",
    "            output=minll(G_in_sparse,matrix(h_in),matrix(p))\n",
    "            print(output['primal objective'])\n",
    "            time4=time.clock()\n",
    "            if output['status'] == 'optimal':\n",
    "                time_generic_list.append(time4-time3)\n",
    "            else:\n",
    "                time_generic_list.append(-1.0*(time4-time3))\n",
    "\n",
    "         loss = 0.0\n",
    "         for i in range(len(classifier_probs)):\n",
    "            if i<n_1:\n",
    "                loss+=np.log(classifier_probs[i][0])\n",
    "            elif i>=n_1:\n",
    "                loss+=np.log(classifier_probs[i][1])\n",
    "         loss=-1*loss/len(classifier_probs)\n",
    "         print('Log loss for eps %s is %s' % (eps[m],loss))\n",
    "         loss_final.append([eps[m],loss,subsample_size])\n",
    "         loss_list.append(loss)\n",
    "         time_list.append(time2-time1)\n",
    "\n",
    "    loss_avg=np.mean(loss_list)\n",
    "    loss_var=np.var(loss_list)\n",
    "    time_avg=np.mean(time_list)\n",
    "    time_var=np.var(time_list)\n",
    "    num_edges_avg=np.mean(num_edges_list)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " edge_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, -0.0, 2500],\n",
       " [1, -0.0, 2500],\n",
       " [2, 0.0005004024235381879, 2500],\n",
       " [3, 0.032377251425240644, 2500],\n",
       " [4, 0.3458605759331595, 2500],\n",
       " [5, 0.6780286674263256, 2500],\n",
       " [6, 0.6931472005678916, 2500],\n",
       " [7, 0.6931472005678916, 2500],\n",
       " [8, 0.6931472005678916, 2500],\n",
       " [9, 0.6931472005678916, 2500],\n",
       " [10, 0.6931472005678916, 2500]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36205.375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_edges_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47582873, 0.52417127],\n",
       "       [0.47582873, 0.52417127],\n",
       "       [0.75      , 0.25      ],\n",
       "       ...,\n",
       "       [0.47582873, 0.52417127],\n",
       "       [0.47582873, 0.52417127],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classifier_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.84659166, 11.05460773, 11.2006371 , ...,  9.84820743,\n",
       "        10.14525417, 10.79192316],\n",
       "       [10.67833272,  9.94280066,  9.73412725, ...,  8.24689659,\n",
       "         8.65168788,  8.87287206],\n",
       "       [12.37230167, 11.21644177, 11.46179988, ..., 10.37937653,\n",
       "        11.40822194, 11.02153078],\n",
       "       ...,\n",
       "       [ 9.76150589,  9.54798964,  8.89217567, ...,  6.50025759,\n",
       "         7.60278283,  8.02944217],\n",
       "       [ 9.98775567,  9.52234467,  9.70566369, ...,  7.67758312,\n",
       "         8.55505094,  9.23188147],\n",
       "       [11.19455228,  9.94081061, 10.76381081, ...,  9.55363982,\n",
       "        10.58924977, 10.04681698]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
